\wde{Classification}{
    Given a dataset $S$, the goal is to learn a function $h$ that maps 
    input variables $x$ to $y$, where $y$ is a class label. 
}
\wde{Linear Seperability}{
    A dataset is linearly seperable if there exists a hyperplane that can seperate the data points
}
\wde{Training of Classification}{
    Find parameters $\theta$ such that the zero-one loss is minimised
}
\wde{Logistic Regression}{
    A linear classifier that models the probability of a class label.
    The model is given by
    $$
    p(y|x, \theta) = \sigma(-y\theta^Tx) = \frac{1}{1 + \exp(-y (\dotp{w}{x} + b))}
    $$
    where $\sigma$ is the sigmoid function.
}
\wde{Maximum Likelihood Estimation (MLE)}{
    The MLE of the parameters $\theta$ is given by
    $$
    \hat \theta = \argmax_\theta \prod_{i=1}^n p(y_i|x_i, \theta)
    $$
}
\wde{Log Likelihood}{
    The log likelihood is applying log to the MLE
    to obtain a more numerically stable solution:
    $$
    L = \sum_{i=1}^n \log p(y_i|x_i, \theta)
    $$
}   
\wa{Training of Logistic Regression}{
    The training of logistic regression is done by maximising the log likelihood $L$ of $w$ and $b$.
    $$
    L = \sum_{i=1}^n \log p(y_i|x_i, \theta)
    $$
    There are no closed-form solutions to this problem, so iterative methods like gradient ascent are used
    which is equivalent to minimising the negative log likelihood. 
}
\wde{Multiclass Classification}{
    Using the softmax function, we can extend logistic regression to multiclass classification.
    Softmax is defined as
    $$
    \operatorname{softmax}\left( \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_k \end{bmatrix} \right) = \begin{bmatrix} \frac{e^{z_1}}{\sum_{i=1}^k e^{z_i}} \\ \frac{e^{z_2}}{\sum_{i=1}^k e^{z_i}} \\ \vdots \\ \frac{e^{z_k}}{\sum_{i=1}^k e^{z_i}} \end{bmatrix}
    $$
}
\wde{Support Vector Machine (SVM)}{
    A linear classifier that finds the hyperplane that maximises the margin between the classes. It 
    does so by solving the following optimisation problem:
    $$
    \min_{w} \frac{1}{2} \norm{w}^2 \quad \text{s.t.} \quad y_i(w^Tx_i + b) \geq 1
    $$
    So the lagrangian is 
    $$
    L(\alpha, w) = \frac{1}{2} \norm{w}^2 - \sum_{i=1}^n \alpha_i(y_i(w^Tx_i + b) - 1)
    $$
}
\wde{Kernel}{
    A kernel $k : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ is defined as 
    $$
    k(x, x') = \phi(x)^T\phi(x')
    $$ for some feature function $\phi$.
    It is a measure of similarity between two points in the input space.
}
\we{Common Kernels}{
    \begin{itemize}
        \item Linear: $k(x, x') = x^Tx'$
        \item Polynomial: $k(x, x') = (x^Tx' + 1)^d$
        \item Gaussian: $k(x, x') = \exp\left( -\frac{\norm{x - x'}^2}{2\sigma^2} \right)$
        \item Hyperbolic Tangent: $k(x, x') = \tanh(kx^Tx' + c)$    
    \end{itemize}
}
\wde{Making Kernels}{
    A kernel should satisfy 
    \begin{itemize}
        \item $k(x,z) = \inner{\phi(x)}{\phi(z)} = \inner{\phi(z)}{\phi(x)} = k(z,x)$ (symmetric)
        \item $k(z,x)^2 = k(x,x)k(z,z)$
        \item 
        \begin{align*}
        K = 
        \begin{bmatrix} k(x_1, x_1) & \cdots & k(x_1, x_n) \\ \vdots & \ddots & \vdots \\ k(x_n, x_1) & \cdots & k(x_n, x_n) \end{bmatrix}
        \end{align*}
        is positive semi-definite
    \end{itemize}
}

\wt{Mercer's Theorem}{
    Supoose $k$ is a continuous symmetric non-negative definite kernel, then $k$ can be expressed as 
    $$
    k(x,z) = \sum_{i=1}^\infty \lambda_i \phi_i(x)\phi_i(z)
    $$
    where $\{\phi \}$ are eigen-functions, $\norm{\phi_i} = 1$ and $\{\lambda_i\}$ are positive eigen values $\lambda_i \geq 0$.    
}

\wde{Kernel Trick}{
    The kernel trick is a method to transform the input space into a higher dimensional space
    without explicitly computing the transformation. This is done by using a kernel function
    $$
    k(x, x') = \phi(x)^T\phi(x')
    $$
    where $\phi$ is the transformation function.
}
