\wde{Kullback-Leibler Divergence}{
    Measure how well one probability distribution approximates another 
    $$KL(p\Vert q) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q(x)}\right] = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}$$ 
    Not a metric as it is not symmetric $KL(p\Vert q) \neq KL(q\Vert p)$
}
\wde{General Latent Variable Model}{
    Two sets of random variables: observed variables $X$ and unseen/hidden/latent/missing variables $Z$.
}
\wde{
    Expectation Maximisation 
}{
    If we can't solve an optimisation problem directly or if we have missing data, we can use the EM algorithm.
    It alternates between inferring the missing values given the parameters (E-step) and optimising the parameters given the filled in data (M-step).
}
\wa{EM Algorithm}{
    \begin{algorithmic}
        \State Initialise $\theta^{old}$
        \While{not converged}
            \State E-step: Let $q^*(Z) = p(Z \vert X, \theta^{old})$
            \State Compute $J(\theta) = \sum_Z q^*(Z) \log \frac{p(X, Z \vert \theta)}{q^*(Z)}$
            \State M-step: $\theta^{new} = \arg\max_\theta J(\theta)$
        \EndWhile
    \end{algorithmic}
}

\wde{K-means Distortion}{
    $$
    J = \sum^N_{n = 1}\sum^K_{k = 1} r_{nk} \Vert x_n - \mu_k \Vert^2
    $$
}
\wde{K-means Steps}{
 \begin{itemize}
 \item Randomly initialise $\mu_{k = 1,\dots, K}$ 
 \item Minimise $J$ with respect to $r_{nk}$ (Expectation)
 \item Minimise $J$ with respect to $\mu_{k}$ (Maximisation) 
 \item Repeat until convergence
 \end{itemize}
}
\wde{K-means Solutions}{
    $$\mu_k = \frac{\sum_n r_{nk} x_n}
    % {\sum_n r_{nk}}, r_{nk} = \mathbb{I}
    {\underbrace{\sum_n r_{nk}}_{\text{Mean of cluster }}}, 
    (k = \arg\min_j \Vert x_n - \mu_j \Vert ^2)$$
}
\wde{GMM Probabilities}{
    $$p(z_k = 1) = \pi_k, p(\mathbf{z}) = \prod_{k = 1}^K \pi_k^{z_k}, p(\mathbf{x} \vert \mathbf{z}) = \prod_{k = 1}^K \mathcal{N}(x | \mu_k, \Sigma_k)^{z_k}$$
}
\wde{GMM Responsibility Function}{
    $$r_{nk} = p(z_k = 1 \vert x_n) = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j = 1}^K \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}$$
}