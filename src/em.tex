\wde{
    Expectation Maximisation 
}{
    If can't solve an optimisation problem directly, EM is used. It yields a lower bound of the original loss. It utilises Jensen's inequality.
}

\wde{K-means Distortion}{
    $$
    J = \sum^N_{n = 1}\sum^K_{k = 1} r_{nk} \Vert x_n - \mu_k \Vert^2
    $$
}
\wde{K-means Steps}{
 \begin{itemize}
 \item Randomly initialise $\mu_{k = 1,\dots, K}$ 
 \item Minimise $J$ with respect to $r_{nk}$ (Expectation)
 \item Minimise $J$ with respect to $\mu_{k}$ (Maximisation) 
 \item Repeat until convergence
 \end{itemize}
}
\wde{K-means Solutions}{
    $$\mu_k = \frac{\sum_n r_{nk} x_n}
    % {\sum_n r_{nk}}, r_{nk} = \mathbb{I}
    {\underbrace{\sum_n r_{nk}}_{\text{Mean of cluster }}}, 
    (k = \arg\min_j \Vert x_n - \mu_j \Vert ^2)$$
}
\wde{GMM Probabilities}{
    $$p(z_k = 1) = \pi_k, p(\mathbf{z}) = \prod_{k = 1}^K \pi_k^{z_k}, p(\mathbf{x} \vert \mathbf{z}) = \prod_{k = 1}^K \mathcal{N}(x | \mu_k, \Sigma_k)^{z_k}$$
}
\wde{GMM Responsibility Function}{
    $$r_{nk} = p(z_k = 1 \vert x_n) = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j = 1}^K \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}$$
}
\wde{Kullback-Leibler Divergence}{
    Measure how well one probability distribution approximates another 
    $$KL(p\Vert q) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q(x)}\right] = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}$$ 
    Not a metric as it is not symmetric $KL(p\Vert q) \neq KL(q\Vert p)$
}