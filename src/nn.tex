\wde{Perceptron (Single-layer Neural Network)}{
    A simple linear classifier that learns a weight vector $w$ to classify data points.
    The perceptron training algorithm can be used to learn the weights.
    Can model logical functions like AND, OR and NOT.
}
\wt{Universal Approximation Theorem}{
    A single-output node NN with  a single hidden layer with 
    finite neurons can app    finite neurons can approximate continuous (and discontinuous) functions
} 
\wde{Multi-layer Perceptron (MLP) / Feedforward Neural Netword (FNN)}{
    A neural network with multiple layers of perceptrons.
    The network is feedforward as there are no cycles in the network. 
    It can model complex decision boundaries (piecewise linear) [XOR, etc]
    The perceptron training algorithm can not be used to learn the weights.

    The notation $w_{ij}^{(l)}$ denotes the weight from the $i$th neuron in layer $l-1$ to the $j$th neuron in layer $l$.
}
\wde{Activation Function}{
    Allows for non-linear decision boundaries. They should 
    be differentiable. Also controls the output range to 
    a specific range. Common activation functions are:
    \begin{itemize}
        \item Sigmoid: $\sigma(x) = \frac{1}{1 + e^{-x}}$
        \item Hyperbolic Tangent (tanh): $\tanh(x) = \frac{1-\exp(-2x)}{1+\exp(-2x)}$
        \item ReLU: $f(x) = \max(0, x)$ (faster than tanh)
        \item Leaky ReLu solves the 'dying ReLU' problem
    \end{itemize}
}
\wde{Computation Graph}{
    Represents computation as a directed graph comprising of simple operations 
    on vectors and matrices which allows for automatic differentiation.
}
\wde{Training of Single-Layer Network}{
    Given an error function $E_n = \frac{1}{2}(y_n - \hat y_n)^2$ (MSE),
    where $\hat y_n = g(a_{nk})$ (some activation function $g$) [usually sigmoid],
    and $a_{nk} = w_k^Tx_n$ (weighted sum of inputs),
    the weights are updated using the gradient descent algorithm
    $$
    w_{t+1} = w_t - \eta \pderivative{E_n}{w_k}
    $$    
    By chain rule,
    $$
    \pderivative{E_n}{w_k} = \pderivative{E_n}{\hat y_n} \pderivative{\hat y_n}{a_{nk}} \pderivative{a_{nk}}{w_k}
    $$
}