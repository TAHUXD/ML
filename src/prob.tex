\wde{
    Gaussian Distribution
}{
    We write $x \sim \mathcal{N}(\mu, \Sigma)$ to denote that $x$ is a random variable with mean $\mu$ and covariance $\Sigma$.
    It means that the probability density function of $x$ is given by
    $$
    p(x) = \frac{1}{2\pi^{d/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
    $$
}

\wde{Conditional Probability}{
    The probability of an event $A$ given that another event $B$ has occurred is denoted by $p(A|B)$.
    $$
    p(A|B) = \frac{p(A,B)}{p(B)}
    $$ 
}
\wde{Joint Probability}{
    The probability of two events $A$ and $B$ occurring together is denoted by $p(A,B)$.
    There is an useful identity that relates joint probability to conditional probability.
    $$
    p(A,B) = p(A|B)p(B) = p(B|A)p(A)
    $$
    This can be sequentially extended to more variables i.e. 
    $$
    p(A,B,C) = p(A|B,C)p(B,C) = p(A|B,C)p(B|C)p(C)
    $$
}

\wde{Statistical Independence}{
    Two variables $x$ and $y$ are independent if 
    $$ 
    p(x,y) = p(x)p(y)
    $$
    Equivalently,
    $$
    p(x|y) = p(x)
    $$.
    The independence of $x$ and $y$ is denoted by $x \perp y$.
}
\wde{Statistical Independence [general]}
{
    If $\{x_1 , \ldots , x_n\} \perp \{y_1 , \ldots , y_m\}$ then
    $$
    p(x_1, \cdots, x_n, y_1, \cdots, y_m) = p(x_1, \cdots, x_n)p(y_1, \cdots, y_m)
    $$
}
\we{
    Factorisation of a joint distribution
}{
    Suppose $x \in \mathcal{X}, y \in \mathcal{Y}, z \in \mathcal{Z}$. If $\{x,y\} \perp z$ then
    $$
    p(x,y,z) = p(x,y)p(z)
    $$
    The original joint distribution (of size $|\mathcal{X}| \times |\mathcal{Y}| \times |\mathcal{Z}|$) can be factorised into two distributions of size $|\mathcal{X}| \times |\mathcal{Y}|$ and $|\mathcal{Z}|$.
}
\wde{Mutual Independence}{
    A set of variables $\{x_1, \ldots, x_n\}$ are mutually independent if
    $$
    p(x_1, \ldots, x_n) = \prod_{i=1}^n p(x_i)
    $$
}
\wde{Pairwise Independence}{
    A set of variables $\{x_1, \ldots, x_n\}$ are pairwise independent if
    $$
    p(x_i, x_j) = p(x_i)p(x_j)
    $$
    for all $i \neq j$.
}
\wt{Mutual Independence implies Pairwise Independence}{
    If a set of variables $\{x_1, \ldots, x_n\}$ are mutually independent, then they are pairwise independent.
    Converse is not true!
}
\wde{Conditional Independence}{
    The variables $x$ and $y$ are conditionally independent given $z$ if 
    $$
    p(x,y|z) = p(x|z)p(y|z)
    $$
    This is denoted by $x \perp y | z$.
}
\wde{Marginilisation}{
    The marginal distribution of $x$ is obtained by summing out all other variables.
    \begin{align*}
        p(x) &= \sum_y p(x,y) \\ 
        p(x | z) &= \sum_y p(x,y | z) \\
        p(x,y | z) &= p(x | z)p(y | z)
    \end{align*}
}
\wde{Bayes Rule}{
    Bayes rule is a way to invert conditional probabilities.
    $$
    p(x|y) = \frac{p(y|x)p(x)}{p(y)}
    $$
}
\wde{Chain Rule}{
    Any joint distribution can be factorised into a product of conditional distributions.
    $$
    p(x_1, \ldots, x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\cdots p(x_n|x_1,\ldots,x_{n-1})
    $$
}
\wde{Conditional Probability Chain Rule}{
    The chain rule can be generalised to conditional probabilities.
    $$
    p(x_1, \ldots, x_n | y) = p(x_1 | y)p(x_2 | x_1, y)\cdots p(x_n | x_1, \ldots, x_{n-1}, y)
    $$
}